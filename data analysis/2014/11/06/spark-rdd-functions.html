<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Hyunje's Blog" />
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Spark RDD의 함수 동작 방식 | Hyunje’s Blog</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Spark RDD의 함수 동작 방식" />
<meta name="author" content="Hyunje" />
<meta property="og:locale" content="ko" />
<meta name="description" content="이 글은 Spark의 RDD에 존재하는 함수들이 어떤 방식으로 동작되는지에 대한 글입니다. Spark 의 공식 Documentation을 일부 번역하였습니다. 또한, Java 기준으로 설명 할 것이며, Scala 로 된 버전은 추후에 추가 작성할 계획입니다. 번역하기에 적절치 않은 용어들은 영문 단어 그대로 남겨놓았습니다." />
<meta property="og:description" content="이 글은 Spark의 RDD에 존재하는 함수들이 어떤 방식으로 동작되는지에 대한 글입니다. Spark 의 공식 Documentation을 일부 번역하였습니다. 또한, Java 기준으로 설명 할 것이며, Scala 로 된 버전은 추후에 추가 작성할 계획입니다. 번역하기에 적절치 않은 용어들은 영문 단어 그대로 남겨놓았습니다." />
<meta property="og:site_name" content="Hyunje’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2014-11-06T14:09:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Spark RDD의 함수 동작 방식" />
<script type="application/ld+json">
{"description":"이 글은 Spark의 RDD에 존재하는 함수들이 어떤 방식으로 동작되는지에 대한 글입니다. Spark 의 공식 Documentation을 일부 번역하였습니다. 또한, Java 기준으로 설명 할 것이며, Scala 로 된 버전은 추후에 추가 작성할 계획입니다. 번역하기에 적절치 않은 용어들은 영문 단어 그대로 남겨놓았습니다.","headline":"Spark RDD의 함수 동작 방식","dateModified":"2014-11-06T14:09:00+00:00","datePublished":"2014-11-06T14:09:00+00:00","url":"/data%20analysis/2014/11/06/spark-rdd-functions.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/data%20analysis/2014/11/06/spark-rdd-functions.html"},"author":{"@type":"Person","name":"Hyunje"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  
  <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.5/build/pure-min.css" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.5/build/grids-responsive-min.css">
  <link rel="stylesheet" href="/assets/css/open-color.css">
  <link rel="stylesheet" href="/assets/css/hydure.css">

  <script async src="https://use.fontawesome.com/releases/v5.0.12/js/all.js"></script>

  <!-- for mathjax support -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>
  <script type="text/javascript" async src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

<link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">
</head>


  <body>
    <div id="layout" class="pure-g">
      
      
      <div class="sidebar pure-u-1 pure-u-md-1-4" style="background-image: url(https://cdn.jsdelivr.net/gh/zivong/jekyll-theme-hydure@master/cover.jpg);">
        <div class="sidebar-shield">
          <header class="header">
  <a class="brand-title" href="/">Hyunje's Blog</a>
  <p class="brand-tagline">개발과 일상에 대한 블로그입니다.</p>

  
    <nav class="nav pure-menu">
      <ul class="pure-menu-list">
      
        <li class="nav-item pure-menu-item">
          <a href="/" class="pure-menu-link ">
            Home
          </a>
        </li>
      
        <li class="nav-item pure-menu-item">
          <a href="/about/" class="pure-menu-link ">
            About
          </a>
        </li>
      
        <li class="nav-item pure-menu-item">
          <a href="/categories/" class="pure-menu-link ">
            Categories
          </a>
        </li>
      
        <li class="nav-item pure-menu-item">
          <a href="/tags/" class="pure-menu-link ">
            Tags
          </a>
        </li>
      
        <li class="nav-item pure-menu-item">
          <a href="/archive/" class="pure-menu-link ">
            Archive
          </a>
        </li>
      
      </ul>
    </nav>
  

  
    <div class="social pure-menu pure-menu-horizontal">
      <ul class="social-list pure-menu-list">
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="mailto:retriever89@gmail.com" target="_blank">
              <i class="fas fa-envelope" title="Email"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://github.com/RetrieverJo" target="_blank">
              <i class="fab fa-github" title="GitHub"></i>
            </a>
          </li>
        
      </ul>
    </div>
  
</header>

        </div>
      </div>
      <div class="content pure-u-1 pure-u-md-3-4">
        <article class="post">
  
    <div class="post-meta">
      <ul class="post-categories"><li>
            <a class="post-category" href="/categories/#data-analysis">Data Analysis</a></li></ul>
    </div>
  
  <h1 class="post-title">Spark RDD의 함수 동작 방식</h1>
  <div class="post-meta">
    <time datetime="2014-11-06T14:09:00+00:00" itemprop="datePublished">
      06 Nov 2014
    </time><span> • </span>
      
        <span itemprop="author" itemscope itemtype="http://schema.org/Person">
          Hyunje
        </span></div>
  
  <p>이 글은 Spark의 RDD에 존재하는 함수들이 어떤 방식으로 동작되는지에 대한 글입니다. <br />
<a href="http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds">Spark 의 공식 Documentation</a>을 일부 번역하였습니다.<br />
또한, Java 기준으로 설명 할 것이며, Scala 로 된 버전은 추후에 추가 작성할 계획입니다.<br />
번역하기에 적절치 않은 용어들은 영문 단어 그대로 남겨놓았습니다.</p>
<hr />

<p><br /></p>

<h3 id="warning">WARNING</h3>
<p>이 글은 최신버전을 기준으로 설명된 글이 아닙니다. 최신 버전을 대상으로 하였을 때와 설치 과정 혹은 출력 결과가 다를 수 있습니다.</p>

<p><br /></p>

<h3 id="spark의-rdd">Spark의 RDD</h3>
<p>Spark는 Resilient Distributed Dataset, RDD 로 구성되어 있습니다. 이 RDD는 분산 형태로 처리 가능한 fault-tolerant collection 입니다. Spark에서 RDD를 생성하는 방법은 두 가지가 있습니다. 첫 번째로는 Driver 프로그램에서 이미 존재하는 colleciton을 <em>parallelizing</em> 시키는 방법이 있고, 다른 방법으로는 HDFS나 HBase혹은 Hadoop InputFormat 으로 수행 가능한 어떠한 데이터 소스를 <em>referencing</em>하는 방법이 있습니다.</p>

<p><br /></p>

<h5 id="parallelized-collections">Parallelized Collections</h5>
<p>Java에서 Parallelized Collection을 생성하기 위해서는 <code class="language-plaintext highlighter-rouge">JavaSparkContext</code>클래스에 존재하는 <code class="language-plaintext highlighter-rouge">parallelize</code> 함수에 Driver 프로그램에서 사용한 Collection을 파라미터로 넘겨주면 됩니다. Collection에 존재하는 엘리먼트들은 Distributed Dataset으로 복사되고, 분산 형태로 연산됩니다. 1 에서 5 까지 값을 갖는 리스트를 parallelized collection으로 생성하는 방법은 다음 예와 같습니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">List</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="nc">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">5</span><span class="o">);</span>
<span class="nc">JavaRDD</span><span class="o">&lt;</span><span class="nc">Integer</span><span class="o">&gt;</span> <span class="n">distData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">parallelize</span><span class="o">(</span><span class="n">data</span><span class="o">);</span>
</code></pre></div></div>
<p>한번 생성된 distributed dataset(<code class="language-plaintext highlighter-rouge">distData</code>)는 분산 형태로 연산될 수 있습니다. 예를들면, 다음과 같은 코드를 이용하여 리스트에 존재하는 모든 값을 더할 수 있습니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">distData</span><span class="o">.</span><span class="na">reduce</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">);</span>
</code></pre></div></div>
<p>이러한 분산 형태의 연산에 대해서는 아래 Section 에서 설명할 것입니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>이 Documentation에서는 Java 8 에서 제공하는 Lamda 문법을 사용하고 있습니다. Java 8을 사용할 수 없는 상황이어서 lambda 표현식을 사용하지 못할 때는, org.apache.spark.api.java.function 패키지를 구현하여 사용할 수 있습니다. 이에 대해서는 아래 Section에서 설명할 것입니다.
</code></pre></div></div>
<p>parallel collection에서 중요한 파라메터중 하나는 데이터셋을 몇 개의 <code class="language-plaintext highlighter-rouge">slices</code>로 나눌 것인지에 대한 것입니다. Spark는 각 cluster의 조각마다 하나의 태스크를 수행 하게 됩니다. 일반적으로 클러스터에서 각각의 CPU 마다 2 ~ 4 개의 slice를 합니다. Spark는 클러스터를 기반으로 slice의 수를 자동적으로 생성을 시도하지만, <code class="language-plaintext highlighter-rouge">parallelize</code>를 수행할 때 그 개수를 수동으로 지정해 줄 수 있습니다. (e.g. <code class="language-plaintext highlighter-rouge">sc.parallelize(data, 10)</code>)</p>

<p><br /></p>

<h4 id="external-datasets">External Datasets</h4>
<p>Spark에서는 Hadoop과 연관되는 모든 데이터 소스(Local File System, HDFS, Cassandra, HBase, Amazon S3, etc.)를 이용하여 distributed dataset을 생성할 수 있습니다. Spark는 텍스트파일, <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">Sequence File</a>과 모든 Hadoop <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">InputFormat</a>을 지원합니다.</p>

<p>텍스트 파일의 RDD는 <code class="language-plaintext highlighter-rouge">SparkContext</code>의 <code class="language-plaintext highlighter-rouge">textFile</code> 함수를 사용하여 생성될 수 있으며, 이 함수는 파일의 URI를 이용하여 파일에 접근합니다. 그리고 텍스트 파일의 한 라인의 collection으로 읽고, 다음과 같은 형태로 파일을 불러옵니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">JavaRDD</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">distFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">"data.txt"</span><span class="o">);</span>
</code></pre></div></div>
<p>한번 생성이 되면, <code class="language-plaintext highlighter-rouge">distFile</code>은 dataset 연산들을 수행할 수 있게 됩니다. 예를들면 <code class="language-plaintext highlighter-rouge">map</code>과 <code class="language-plaintext highlighter-rouge">reduce</code>를 이용하여 모든 라인의 길이를 더한 갚을 구할때는 다음과 같이 명령어를 수행하면 됩니다.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">distFile</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="n">s</span><span class="o">.</span><span class="na">length</span><span class="o">()).</span><span class="na">reduce</span><span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span>
</code></pre></div></div>
<p>Spark를 이용하여 파일을 읽을 때, 다음 사항들을 참고할 수 있습니다.</p>

<ul>
  <li>Local File System 에 있는 파일에 접근할 때, 반드시 Worker node에서 접근 가능한 경로에 파일이 있어야 한다. 모든 Worker에게 파일을 복사하거나, network-mounted 로 공유된 파일 시스템을 사용해야 합니다.<br /><br /></li>
  <li>Spark의 file-based 입력 함수는 폴더, 압축파일과 와일드카드 표현을 포함하여 사용될 수 있습니다. 예를 들면 <code class="language-plaintext highlighter-rouge">textFile("/my/directory")</code>, <code class="language-plaintext highlighter-rouge">textFile("/my/directory/*.txt")</code>, <code class="language-plaintext highlighter-rouge">textFile("/my/directory/*/gz")</code>가 모두 가능합니다.<br /><br /></li>
</ul>

<p>Spark는 텍스트 파일 이외에도 많은 데이터 포맷을 제공합니다.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">JavaSaprkContext.wholeTextFiles</code>는 하나의 폴더 안에 존재하는 텍스트 파일을 읽습니다. 그리고 그것을 &lt;파일이름, 내용&gt; 형태의 쌍으로 리턴합니다. 이것은 한 파일을 읽어 각각의 줄에 대해 처리하는 <code class="language-plaintext highlighter-rouge">textFile</code>과는 다른 형태입니다.<br /><br /></li>
  <li><a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>를 읽기 위해서는 SparkContext의 <code class="language-plaintext highlighter-rouge">sequenceFile[K,V]</code> 함수를 사용해야합니다. K 와 V는 해당 파일에서 사용하고 있는 Key와 Value의 타입입니다. 이것들은 Hadoop의 IntWritable 과 Text 클래스와 같이 <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Writable.html">Writable</a>클래스의 subclass여야 합니다. Spark에서는 이러한 과정을 돕기 위해 몇 개의 일반적인 Writable 타입에 대해 native type을 제공합니다. 예를들어 <code class="language-plaintext highlighter-rouge">sequenceFile[Int,String]</code>을 사용하면, 이것은 <code class="language-plaintext highlighter-rouge">IntWritable</code>과 <code class="language-plaintext highlighter-rouge">Text</code>로 인식됩니다.<br /><br /></li>
  <li>다른 Hadoop InputFormat을 사용하기 위해서는 <code class="language-plaintext highlighter-rouge">JavaSparkContext.HadoopRDD</code>함수를 사용해야 합니다. 이 함수는 임의의 JobConf와 InputFormat 클래스, Key 클래스, Value 클래스를 받습니다. Hadoop 에서 입력 소스를 지정하는 과정과 같은 방식으로 지정을 해야 합니다. 또한 새 맵리듀스 API(org.apache.hadoop.mapreduce 패키지에 존재하는 API)를 사용하기 위해서는<code class="language-plaintext highlighter-rouge">JavaSparkContext.newHadoopRDD</code>를 사용해야합니다.<br /><br /></li>
  <li><code class="language-plaintext highlighter-rouge">JavaRDD.saveAsObjectFile</code>과 <code class="language-plaintext highlighter-rouge">JavaSparkContext.objectFile</code>은 Serialized된 자바 객체 형태로 출력하는 것을 지원합니다. 이것은 Avro와 같이 특수화 된 형태보다는 효율적이지 않지만 간단한 형태로 어떠한 RDD도 저장 가능합니다.</li>
</ul>

<p><br /></p>

<h4 id="rdd-operations">RDD Operations</h4>
<p>추후 번역</p>


  
    <div class="post-meta">
      <i class="post-tags-icon fas fa-tag"></i>
      <ul class="post-tags"><li>
            <a class="post-tag" href="/tags/#spark">spark</a></li><li>
            <a class="post-tag" href="/tags/#spark-rdd">spark-rdd</a></li></ul>
    </div>
  

  
</article>


        <footer class="footer pure-g">
  <div class="pure-u-1 pure-u-md-1-2">
    <small>
      &copy;&nbsp;<time datetime="2014-10-24T04:19:00+00:00">2014</time>-<time datetime="2021-07-03T09:29:54+00:00">2021</time>&nbsp;<a href="https://hyunje.com" target="_blank">Hyunje Jo</a>. All right reserved.
    </small>
  </div>

  <div class="pure-u-1 pure-u-md-1-2">
    <small>
      Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> & <a href="https://github.com/zivong/jekyll-theme-hydure" target="_blank">Hydure</a>
    </small>
  </div>
</footer>

      </div>
    </div>

    

    
  </body>
</html>
