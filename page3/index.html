<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Hyunje Blog &#8211; Hyunje Blog</title>

<meta name="google-site-verification" content="KstVPrbjGTMueNzKiyCurwLlZ0wNcfscVNW4KmZtXC4" />

<meta name="description" content="Blog for Hyunje">
<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Hyunje Blog">
<meta property="og:description" content="Blog for Hyunje">
<meta property="og:url" content="/page3/index.html">
<meta property="og:site_name" content="Hyunje Blog">





<link rel="canonical" href="/page3/">
<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Hyunje Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">
<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
<link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
<link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/images/apple-touch-icon-144x144-precomposed.png">




<style type="text/css">body {background-image:url(/images/white.jpg);}</style>


</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="/images/avatar.jpg" alt="Hyunje Jo photo" class="author-photo">
					<h4>Hyunje Jo</h4>
					<p>Bigdata-technology based Machine Learning & Data Analysis</p>
				</li>
				<li><a href="/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:retriever89@gmail.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				<li>
					<a href="https://facebook.com/RetrieverJo"><i class="fa fa-fw fa-facebook"></i> Facebook</a>
				</li>
				
				
				<li>
					<a href="https://github.com/retrieverJo"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="/posts/">All Posts</a></li>
				<li><a href="/categories/">All Categories</a></li>
				<li><a href="/tags/">All Tags</a></li>
			</ul>
		</li>
		<!--
		
	    
	        
	    
	    <li><a href="/theme-setup/" >Theme Setup</a></li>
	  
	    
	        
	        
	    <li><a href="http://mademistakes.com" target="_blank">External Link</a></li>
	  
	  	-->
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="">Hyunje Jo</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="/images/bg0.jpg" alt="Hyunje Blog">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Hyunje Blog</h1>
      <h2>Hyunje Blog</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="/data%20analysis/2015/07/04/advanced-analytics-with-spark-ch2/" title="Spark & 머신 러닝 - Introduction to Spark"><img src="/images/bg1.jpg" alt="Spark & 머신 러닝 - Introduction to Spark"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2015-07-04T11:09:00-04:00"><a href="/data%20analysis/2015/07/04/advanced-analytics-with-spark-ch2/">July 04, 2015</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Hyunje Jo">Hyunje Jo</a></span></span>
      <!--
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~1 minute
      </span>
      -->
      <!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/data%20analysis/2015/07/04/advanced-analytics-with-spark-ch2/" rel="bookmark" title="Spark & 머신 러닝 - Introduction to Spark" itemprop="url">Spark & 머신 러닝 - Introduction to Spark</a></h1>
    
  </header>
  <div class="entry-content" style="overflow:hidden; height:500px;">
    <p>이 글에서는 Spark가 어떤 형태로 동작하는지 설명하고 간단한 예시를 통해 Spark에 적응하는 과정을 설명한다.</p>

<p>이 포스트는 <a href="http://shop.oreilly.com/product/0636920035091.do">Advanced Analytics with Spark</a>을 정리한 글이다.</p>

<p>이 글에서 다루고자 하는 내용은 Chapter 2이다. Chapter 1은 빅데이터에 대한 개략적인 얘기와, 왜 Spark가 뜨고 있는지, Spark 가 데이터 분석에서 어떠한 역할을 하고 있는지에 대한 설명이 있었다. 그 내용들은 다른 자료들에 많이 있으므로 따로 정리는 하지 않았다.</p>

<p><br>
<br></p>

<h2>Record Linkage</h2>

<p>chapter 2에서는 Record Linkage와 비슷한 작업(ex : <a href="https://en.wikipedia.org/wiki/Extract,_transform,_load">ETL</a>)들을 Spark 로 어떻게 수행하는지에 대해 설명하고, Follow-up 할 수 있도록 하고있다.</p>

<p>실제 데이터를 이용해 분석을 수행할 때 대부분은 수집한 데이터를 그대로 이용하지 않는다. 수집한 데이터를 그대로 이용한다면 잘못된 데이터(값이 비어있거나, 필요없는 데이터가 섞여있거나, 같은 데이터가 중복되어 들어있거나 등등)들이 분석에 그대로 활용되기 때문에 이들을 잘 필터링 해야 한다.</p>

<p>이 책에서 Recoed Linkage는 위의 문제 중 같은 데이터가 다른 형태로 들어있을 때, 그것을 하나의 데이터로 간주하도록 하는 것이라 얘기하고 있다.</p>

<p>이러한 내용들을 수행하기 위해 Record Linkage Comparison Patterns Dataset을 이용한다.</p>

<p><br>
<br></p>

<h2>The Spark Shell and SparkContext</h2>

<p>Spark를 이용하는 방법은 크게 두 가지가 있다. 하나는 Spark Shell 을 통해서 REPL(read-eval-print loop) 형태로 Spark를 이용하는 방법이고, 나머지 방법은 Spark Application 을 IDE를 이용해 작성한 후, 그것을 패키징 하여 Spark Cluster로 Submit하여 수행하는 방법이다.</p>

<p>REPL을 이용하려면 다음과 같은 명령어를 이용해야한다.</p>

<p>(여기서, 개인적인 공부이기 때문에 로컬 클러스터에서 수행함을 가정한다. 또한, Shell은 Scala를 기반으로 작성해야 하기 때문에 Scala 에 대한 이해가 필요하다.)</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>spark-shell --master <span class="nb">local</span><span class="o">[</span>2<span class="o">]</span>
</code></pre></div>
<p>만약 Spark 를 YARN을 이용해 수행시키고 싶다면 다음과 같이 입력한다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>spark-shell --master yarn-client
</code></pre></div>
<p><br></p>

<p>이 챕터에서는 <a href="http://bit.ly/1Aoywaq">http://bit.ly/1Aoywaq</a> 링크의 데이터를 이용하고 있다.
이 데이터를 HDFS로 업로드해야 하기 때문에, 다음 과정을 이용해 데이터를 HDFS로 업로드 한다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>wget http://bit.ly/1Aoywaq -O donation.zip
<span class="nv">$ </span>unzip donation.zip
<span class="nv">$ </span>unzip <span class="s1">&#39;block_*.zip&#39;</span>
<span class="nv">$ </span>hdfs dfs -mkdir /linkage
<span class="nv">$ </span>hdfs dfs -put block_*.csv /linkage
</code></pre></div>
<p>책에서는 (아직은) Spark-shell을 기준으로 설명하고 있다. 본 글 역시 다른 언급이 있지 않는 이상 Spark-shell 을 기준으로 설명할 것이다.</p>

<p>우선, 다음 명령어를 이용해 데이터가 HDFS에 정상적으로 업로드 되고, 그것을 잘 읽어 오는지 확인한다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">rawblocks</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="o">(</span><span class="s">&quot;/linkage&quot;</span><span class="o">)</span>
<span class="o">...</span>
<span class="n">rawblocks</span><span class="k">:</span> <span class="kt">org.apache.spark.rdd.RDD</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="nc">MapPartitionsRDD</span><span class="o">[</span><span class="err">1</span><span class="o">]</span> <span class="n">at</span> <span class="n">textFile</span> <span class="n">at</span> <span class="o">&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">21</span>
</code></pre></div>
<p>Spark Shell 에서는 기본적인 SparkContext 객체에 대한 인스턴스를 하나 제공한다. 그 인스턴스에 대한 접근은 <code>sc</code>로 할 수 있으며, <strong>textFile</strong> 함수를 이용해 HDFS에 저장되어 있는 파일을 읽어올 수 있다.</p>

<p>다음 명령어를 이용해 데이터의 상위 10 줄에 어떤 데이터가 들어있는지 확인한다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">head</span> <span class="k">=</span> <span class="n">rawblocks</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">10</span><span class="o">)</span>
<span class="n">head</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre></div>
<p>그러면 다음과 같은 값이 들어있음을 알 수 있다.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">&quot;id_1&quot;,&quot;id_2&quot;,&quot;cmp_fname_c1&quot;,&quot;cmp_fname_c2&quot;,&quot;cmp_lname_c1&quot;,&quot;cmp_lname_c2&quot;,&quot;cmp_sex&quot;,&quot;cmp_bd&quot;,&quot;cmp_bm&quot;,&quot;cmp_by&quot;,&quot;cmp_plz&quot;,&quot;is_match&quot;
37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE
39086,47614,1,?,1,?,1,1,1,1,1,TRUE
70031,70237,1,?,1,?,1,1,1,1,1,TRUE
84795,97439,1,?,1,?,1,1,1,1,1,TRUE
36950,42116,1,?,1,1,1,1,1,1,1,TRUE
42413,48491,1,?,1,?,1,1,1,1,1,TRUE
25965,64753,1,?,1,?,1,1,1,1,1,TRUE
49451,90407,1,?,1,?,1,1,1,1,0,TRUE
39932,40902,1,?,1,?,1,1,1,1,1,TRUE
</code></pre></div>
<p>이 결과를 보면, csv 데이터의 제일 첫 줄에 각 컬럼이 나타내는 값이 어떤 것인지에 대한 정보가 있다. 이것을 다음과 같은 함수와 명령어를 이용하여 필터링한다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">def</span> <span class="n">isHeader</span><span class="o">(</span><span class="n">line</span><span class="k">:</span><span class="kt">String</span><span class="o">)</span> <span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="o">{</span>
    <span class="n">line</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="s">&quot;id_1&quot;</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">val</span> <span class="n">noheader</span> <span class="k">=</span> <span class="n">rawblocks</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">!</span><span class="n">isHeader</span><span class="o">(</span><span class="n">x</span><span class="o">))</span>
</code></pre></div>
<p>그 결과로, 다음과 같이 컬럼명이 제외된 데이터 10개를 볼 수 있다.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE
39086,47614,1,?,1,?,1,1,1,1,1,TRUE
70031,70237,1,?,1,?,1,1,1,1,1,TRUE
84795,97439,1,?,1,?,1,1,1,1,1,TRUE
36950,42116,1,?,1,1,1,1,1,1,1,TRUE
42413,48491,1,?,1,?,1,1,1,1,1,TRUE
25965,64753,1,?,1,?,1,1,1,1,1,TRUE
49451,90407,1,?,1,?,1,1,1,1,0,TRUE
39932,40902,1,?,1,?,1,1,1,1,1,TRUE
46626,47940,1,?,1,?,1,1,1,1,1,TRUE
</code></pre></div>
<p><br>
<br></p>

<h2>Structuring Data with Tuples and Case Classes</h2>

<p>지금까지 읽은 데이터를 그대로 읽어서 분석에 활용할 수 있지만, 데이터를 파싱하여 활용하면 더욱 쉽게 활용할 수 있다.</p>

<p>데이터는 다음과 같은 형태를 띄고 있다.</p>

<ul>
<li>처음 2개의 Integer 값 : 레코드에서 매칭되는 환자의 ID</li>
<li>9개의 Double 값 : 9가지의 필드에 대한 매칭 스코어(없을 수 있음)</li>
<li>Boolean 값 : 매치 되는지 여부에 대한 판별결과</li>
</ul>

<p>이를 파싱하기 위해 다음과 같은 <a href="http://docs.scala-lang.org/ko/tutorials/tour/case-classes.html">Case Class</a>를 정의하고 필요한 서브 함수를 작성하여 활용한다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">MatchData</span><span class="o">(</span><span class="n">id1</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">id2</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">scores</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">matched</span><span class="k">:</span> <span class="kt">Boolean</span><span class="o">)</span>

<span class="k">def</span> <span class="n">toDouble</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">if</span><span class="o">(</span><span class="s">&quot;?&quot;</span><span class="o">.</span><span class="n">equals</span><span class="o">(</span><span class="n">s</span><span class="o">))</span> <span class="nc">Double</span><span class="o">.</span><span class="nc">NaN</span> <span class="k">else</span> <span class="n">s</span><span class="o">.</span><span class="n">toDouble</span>
<span class="o">}</span>

<span class="k">def</span> <span class="n">parse</span><span class="o">(</span><span class="n">line</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">pieces</span> <span class="k">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="sc">&#39;,&#39;</span><span class="o">);</span>
    <span class="k">val</span> <span class="n">id1</span> <span class="k">=</span> <span class="n">pieces</span><span class="o">.</span><span class="n">apply</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">toInt</span>
    <span class="k">val</span> <span class="n">id2</span> <span class="k">=</span> <span class="n">pieces</span><span class="o">.</span><span class="n">apply</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="n">toInt</span>
    <span class="k">val</span> <span class="n">scores</span> <span class="k">=</span> <span class="n">pieces</span><span class="o">.</span><span class="n">slice</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">11</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">toDouble</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">matched</span> <span class="k">=</span> <span class="n">pieces</span><span class="o">.</span><span class="n">apply</span><span class="o">(</span><span class="mi">11</span><span class="o">).</span><span class="n">toBoolean</span>
    <span class="nc">MatchData</span><span class="o">(</span><span class="n">id1</span><span class="o">,</span> <span class="n">id2</span><span class="o">,</span> <span class="n">scores</span><span class="o">,</span> <span class="n">matched</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div>
<p>다음 명령어를 통해 데이터를 파싱하고, 그 결과를 확인한다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">parsed</span> <span class="k">=</span> <span class="n">noheader</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="n">parse</span><span class="o">(</span><span class="n">line</span><span class="o">))</span>

<span class="n">parsed</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre></div>
<p>그리고, 지금까지 파싱한 결과를 메모리에 cache()시켜놓는다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">parsed</span><span class="o">.</span><span class="n">cache</span><span class="o">()</span>
</code></pre></div>
<p><br>
<br></p>

<h2>Creating Histograms</h2>

<p><a href="https://en.wikipedia.org/wiki/Histogram">Histogram</a> 은 간단히 말하면, 항목별로 개수를 센 결과를 나타낸다고 할 수 있다. 이 절에서는 지금까지 파싱한 데이터가 <code>matched</code> 필드 값의 종류(true, false)별로 얼마나 존재하는지에 대한 히스토그램을 생성할 것이다. 다행히도 Spark의 RDD에서 기본적으로 제공하는 <strong>countByValue</strong>를 이용하면 쉽게 해결할 수 있다.</p>

<p>다음 명령어를 통해 각 값 별로 얼마만큼의 레코드가 존재하는지 쉽게 카운팅 할 수 있다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">matchCounts</span> <span class="k">=</span> <span class="n">parsed</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">md</span> <span class="k">=&gt;</span> <span class="n">md</span><span class="o">.</span><span class="n">matched</span><span class="o">).</span><span class="n">countByValue</span><span class="o">()</span>
</code></pre></div>
<p>다음과 같은 수행 결과로, 손쉽게 <code>matched</code> 필드의 각 값 별 히스토그램을 생성할 수 있다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">matchCounts</span><span class="k">:</span> <span class="kt">scala.collection.Map</span><span class="o">[</span><span class="kt">Boolean</span>,<span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Map</span><span class="o">(</span><span class="kc">true</span> <span class="o">-&gt;</span> <span class="mi">20931</span><span class="o">,</span> <span class="kc">false</span> <span class="o">-&gt;</span> <span class="mi">5728201</span><span class="o">)</span>
</code></pre></div>
<p><br>
<br></p>

<h2>Summary Statistics for Continuous Variables</h2>

<p>앞서 설명된 <strong>countByValue</strong>는 값의 종류에 따라 Histogram 을 생성하는 좋은 방법 중 하나이다. 하지만 Boolean 형태의 값처럼 적은 범위를 갖는 값이 아니라 Continuous Variable, 즉 연속변수와 같은 경우에 사용하기에는 적절하지 않다.</p>

<p>연속변수들에 대해서는 모든 값의 Histogram 을 구하는 것보다 분포에 대한 확률적인 통계 수치(평균, 표준편차, 최대 or 최솟값 등)를 보는 것이 좀 더 간결하게 데이터를 파악할 수 있다.</p>

<p>Spark에서는 이를 위해 <strong>stats</strong>라는 함수를 제공한다. 이 함수를 이용함으로써 손쉽게 특정 변수에 대한 통계적 수치를 출력할 수 있다. 파싱한 값중에 NaN이 들어가 있을 수 있기 때문에, 해당 레코드는 필터링을 수행한 후에 수행시키도록 한다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">java.lang.Double.isNaN</span>
<span class="n">parsed</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">md</span> <span class="k">=&gt;</span> <span class="n">md</span><span class="o">.</span><span class="n">scores</span><span class="o">(</span><span class="mi">0</span><span class="o">)).</span><span class="n">filter</span><span class="o">(!</span><span class="n">isNaN</span><span class="o">(</span><span class="k">_</span><span class="o">)).</span><span class="n">stats</span><span class="o">()</span>
</code></pre></div>
<p>위 코드의 수행 결과로 다음과 같은 결과를 볼 수 있다.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">org.apache.spark.util.StatCounter = (count: 5748125, mean: 0.712902, stdev: 0.388758, max: 1.000000, min: 0.000000)
</code></pre></div>
<p>또한 Scala의 <strong>Range</strong>를 이용하여 scores 배열에 들어있는 모든 변수에 대한 수치값들에 대한 통계치를 구할 수 있다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">stats</span> <span class="k">=</span> <span class="o">(</span><span class="mi">0</span> <span class="n">until</span> <span class="mi">9</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="o">{</span>
    <span class="n">parsed</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">md</span> <span class="k">=&gt;</span> <span class="n">md</span><span class="o">.</span><span class="n">scores</span><span class="o">(</span><span class="n">i</span><span class="o">)).</span><span class="n">filter</span><span class="o">(!</span><span class="n">isNaN</span><span class="o">(</span><span class="k">_</span><span class="o">)).</span><span class="n">stats</span><span class="o">()</span>
<span class="o">})</span>
<span class="n">stats</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre></div>
<p>그 결과는 다음과 같다.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">(count: 5748125, mean: 0.712902, stdev: 0.388758, max: 1.000000, min: 0.000000)
(count: 103698, mean: 0.900018, stdev: 0.271316, max: 1.000000, min: 0.000000)
(count: 5749132, mean: 0.315628, stdev: 0.334234, max: 1.000000, min: 0.000000)
(count: 2464, mean: 0.318413, stdev: 0.368492, max: 1.000000, min: 0.000000)
(count: 5749132, mean: 0.955001, stdev: 0.207301, max: 1.000000, min: 0.000000)
(count: 5748337, mean: 0.224465, stdev: 0.417230, max: 1.000000, min: 0.000000)
(count: 5748337, mean: 0.488855, stdev: 0.499876, max: 1.000000, min: 0.000000)
(count: 5748337, mean: 0.222749, stdev: 0.416091, max: 1.000000, min: 0.000000)
(count: 5736289, mean: 0.005529, stdev: 0.074149, max: 1.000000, min: 0.000000)
</code></pre></div>
<p><br>
<br></p>

<h2>Creating Reusable Code for Computing Summary Statistics</h2>

<p>하지만, 위와 같은 작업은 매우 비효율적이다. scores 배열에 존재하는 값에 대한 각각의 통계치를 계산하기 위해서 <code>parsed RDD</code>를 매번 다시 계산해야 한다. 물론 앞선 과정에서 <code>parsed RDD</code> 를 <strong>cache</strong> 해 놓긴 하였지만, 데이터가 많아지면 많아질 수록 이 작업의 소요시간은 급속도로 증가할 것이다.</p>

<p>이러한 경우에, 어떠한 <code>RDD[Array[Double]]</code>를 인자로 받아, 값이 정상적으로 들어있는 레코드들에 대한 각 인덱스별 <code>StatCounter</code> 를 갖는 클래스 혹은 함수를 작성하는 것을 생각해 볼 수 있다.</p>

<p>또한, 이러한 작업이 분석 과정에서 반복될 때, 매번 해당 코드를 새롭게 작성하는 것보다 다른 파일에 작성하여 그것을 재사용하는 것이 적절한 방식이다. 때문에 다른 파일에 스칼라 코드를 작성하고, Spark에서 그 파일을 불러와 사용하도록 할 것이다. 다음 소스코드를 다른 파일 <code>StatsWithMissing.scala</code>파일에 저장한 후 사용할 것이며, 멤버변수와 함수에 대해서는 코드의 뒷 부분에서 설명할 것이다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">import</span> <span class="nn">org.apache.spark.util.StatCounter</span>

<span class="k">class</span> <span class="nc">NAStatCounter</span> <span class="k">extends</span> <span class="nc">Serializable</span><span class="o">{</span>
    <span class="k">val</span> <span class="n">stats</span><span class="k">:</span> <span class="kt">StatCounter</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StatCounter</span><span class="o">()</span>
    <span class="k">var</span> <span class="n">missing</span><span class="k">:</span> <span class="kt">Long</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="n">add</span><span class="o">(</span><span class="n">x</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span><span class="k">:</span> <span class="kt">NAStatCounter</span> <span class="o">=</span> <span class="o">{</span>
        <span class="k">if</span><span class="o">(</span><span class="n">java</span><span class="o">.</span><span class="n">lang</span><span class="o">.</span><span class="nc">Double</span><span class="o">.</span><span class="n">isNaN</span><span class="o">(</span><span class="n">x</span><span class="o">))</span> <span class="o">{</span>
            <span class="n">missing</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
            <span class="n">stats</span><span class="o">.</span><span class="n">merge</span><span class="o">(</span><span class="n">x</span><span class="o">)</span>
        <span class="o">}</span>
        <span class="k">this</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">merge</span><span class="o">(</span><span class="n">other</span><span class="k">:</span> <span class="kt">NAStatCounter</span><span class="o">)</span><span class="k">:</span> <span class="kt">NAStatCounter</span> <span class="o">=</span> <span class="o">{</span>
        <span class="n">stats</span><span class="o">.</span><span class="n">merge</span><span class="o">(</span><span class="n">other</span><span class="o">.</span><span class="n">stats</span><span class="o">)</span>
        <span class="n">missing</span> <span class="o">+=</span> <span class="n">other</span><span class="o">.</span><span class="n">missing</span>
        <span class="k">this</span>
    <span class="o">}</span>

    <span class="k">override</span> <span class="k">def</span> <span class="n">toString</span> <span class="k">=</span> <span class="o">{</span>
        <span class="s">&quot;stats: &quot;</span> <span class="o">+</span> <span class="n">stats</span><span class="o">.</span><span class="n">toString</span> <span class="o">+</span> <span class="s">&quot; NaN: &quot;</span> <span class="o">+</span> <span class="n">missing</span>
    <span class="o">}</span>
<span class="o">}</span>

<span class="k">object</span> <span class="nc">NAStatCounter</span> <span class="k">extends</span> <span class="nc">Serializable</span> <span class="o">{</span>
    <span class="k">def</span> <span class="n">apply</span><span class="o">(</span><span class="n">x</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NAStatCounter</span><span class="o">().</span><span class="n">add</span><span class="o">(</span><span class="n">x</span><span class="o">)</span>
<span class="o">}</span>
</code></pre></div>
<p>앞서 정의한 <code>NAStatCounter</code> 클래스는 두 개의 멤버 변수를 갖고 있다. <code>stats</code>로 정의된 <code>StatCounter</code> 인스턴스는 immutable이고, <code>missing</code>으로 정의된 <code>Long</code> 변수는 mutable 변수이다. 이 클래스를 <code>Serializable</code> 객체를 상속시킨 이유는, Spark 의 RDD에서 이 객체를 사용하기 위해선 반드시 상속을 시켜주어야 한다. 만약 이 상속을 하지 않으면 RDD에서 에러가 발생한다.</p>

<p>클래스의 첫 번째 함수 <code>add</code>는 새로운 Double 형태의 값을 받아 <code>stats</code>변수가 값을 계속 관측할 수 있도록 한다. 만약 인자로 받은 값이 NaN이면, missing 값을 1 증가시키고, NaN이 아니라면 StatCounter 객체에 기록한다. 두 번째 함수 <code>merge</code>는 다른 NAStatCounter 인스턴스를 매개변수로 받아 지금의 인스턴스와 병합시키는 역할을 한다. 세 번째 함수 toString은 쉽게 NAStatCounter 클래스를 출력하기 위해서 오버라이딩 한 것이다. 스칼라에서는 부모 객체의 함수를 오버라이딩 하기 위해선 반드시 함수 앞에 <code>override</code> 키워드를 추가해야 한다.</p>

<p>그리고 class 정의와 함께 NAStatCounter 객체에 대한 <code>companion object</code>를 함께 정의한다. 스칼라에서 object 키워드는 자바에서의 static method 와 같이 어떤 클래스에 대한 helper method를 제공하는 싱글톤 객체를 선언하는데에 이용된다. 이 경우에서처럼 class 이름과 같은 object를 선언하는 것을 <code>companion object</code>를 선언한다고 하며, 여기서의 <code>apply</code> 함수는 <code>NAStatCounter</code> 클래스에 대한 새 인스턴스를 생성하고, 그 인스턴스를 반환하기 전에 Double 값을 더한다.</p>

<p>정상적으로 로드가 되었다면 다음과 같은 메시지가 출력된다.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">import org.apache.spark.util.StatCounter
defined class NAStatCounter
defined module NAStatCounter
warning: previously defined class NAStatCounter is not a companion to object NAStatCounter.
Companions must be defined together; you may wish to use :paste mode for this.
</code></pre></div>
<p>경고가 출력되어 문제가 생긴것이 아닌가 할 수 있지만, 무시할 수 있는 경고이다.
다음과 같은 예시로 정상적으로 로드되었는지 확인해 볼 수 있다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">nas1</span> <span class="k">=</span> <span class="nc">NAStatCounter</span><span class="o">(</span><span class="mf">10.0</span><span class="o">)</span>
<span class="n">nas1</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="mf">2.1</span><span class="o">)</span>
<span class="k">val</span> <span class="n">nas2</span> <span class="k">=</span> <span class="nc">NAStatCounter</span><span class="o">(</span><span class="nc">Double</span><span class="o">.</span><span class="nc">NaN</span><span class="o">)</span>
<span class="n">nas1</span><span class="o">.</span><span class="n">merge</span><span class="o">(</span><span class="n">nas2</span><span class="o">)</span>
</code></pre></div>
<p>이제 작성한 <code>NAStatCounter</code>클래스를 이용하여 <code>parsed RDD</code>에 들어있는 MatchData 레코드를 처리하자. 각각의 MatchData 인스턴스는 Array[Double] 형태의 매칭 스코어를 포함하고 있다. 배열에 있는 각각의 엔트리마다 <code>NAStatCounter</code> 객체를 생성시켜, 모든 값을 추적하려 한다. 그렇게 하기 위해선 다음과 같은 방식으로 RDD 안에 존재하는 모든 레코드는 Array[Double]을 갖고 있기 때문이 이것을 Array[NAStatCounter]를 갖는 RDD로 변경하면 된다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">nasRDD</span> <span class="k">=</span> <span class="n">parsed</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">md</span> <span class="k">=&gt;</span> <span class="o">{</span>
    <span class="n">md</span><span class="o">.</span><span class="n">scores</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">d</span> <span class="k">=&gt;</span> <span class="nc">NAStatCounter</span><span class="o">(</span><span class="n">d</span><span class="o">))</span>
<span class="o">})</span>
</code></pre></div>
<p>이제, 여러개의 Array[NAStatCounter] 를 하나의 배열로 합치면서 각 인덱스별로 존재하는 NAStatCounter를 병합하면 된다. 이를 위해 다음과 같은 코드를 이용한다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">reduced</span> <span class="k">=</span> <span class="n">nasRDD</span><span class="o">.</span><span class="n">reduce</span><span class="o">((</span><span class="n">n1</span><span class="o">,</span> <span class="n">n2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span>
    <span class="n">n1</span><span class="o">.</span><span class="n">zip</span><span class="o">(</span><span class="n">n2</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">merge</span><span class="o">(</span><span class="n">b</span><span class="o">)</span> <span class="o">}</span>
<span class="o">})</span>
<span class="n">reduced</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre></div>
<p>그 결과로 다음과 같이 모든 매칭 스코어에 대한 인덱스별 통계 수치를 구할 수 있다.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">stats: (count: 5748125, mean: 0.712902, stdev: 0.388758, max: 1.000000, min: 0.000000) NaN: 1007
stats: (count: 103698, mean: 0.900018, stdev: 0.271316, max: 1.000000, min: 0.000000) NaN: 5645434
stats: (count: 5749132, mean: 0.315628, stdev: 0.334234, max: 1.000000, min: 0.000000) NaN: 0
stats: (count: 2464, mean: 0.318413, stdev: 0.368492, max: 1.000000, min: 0.000000) NaN: 5746668
stats: (count: 5749132, mean: 0.955001, stdev: 0.207301, max: 1.000000, min: 0.000000) NaN: 0
stats: (count: 5748337, mean: 0.224465, stdev: 0.417230, max: 1.000000, min: 0.000000) NaN: 795
stats: (count: 5748337, mean: 0.488855, stdev: 0.499876, max: 1.000000, min: 0.000000) NaN: 795
stats: (count: 5748337, mean: 0.222749, stdev: 0.416091, max: 1.000000, min: 0.000000) NaN: 795
stats: (count: 5736289, mean: 0.005529, stdev: 0.074149, max: 1.000000, min: 0.000000) NaN: 12843
</code></pre></div>
<p>또한 다음과 같이 <code>statsWithMissing</code>함수를 정의하고 이를 사용함으로써 좀 더 고급지게(?) 처리할 수 있다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">def</span> <span class="n">statsWithMissing</span><span class="o">(</span><span class="n">rdd</span><span class="k">:</span> <span class="kt">RDD</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Double</span><span class="o">]])</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">NAStatCounter</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">nastats</span> <span class="k">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitions</span><span class="o">((</span><span class="n">iter</span><span class="k">:</span> <span class="kt">Iterator</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Double</span><span class="o">]])</span> <span class="k">=&gt;</span> <span class="o">{</span>
        <span class="k">val</span> <span class="n">nas</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">NAStatCounter</span><span class="o">]</span> <span class="k">=</span> <span class="n">iter</span><span class="o">.</span><span class="n">next</span><span class="o">().</span><span class="n">map</span><span class="o">(</span><span class="n">d</span> <span class="k">=&gt;</span> <span class="nc">NAStatCounter</span><span class="o">(</span><span class="n">d</span><span class="o">))</span>
        <span class="n">iter</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">arr</span> <span class="k">=&gt;</span> <span class="o">{</span>
            <span class="n">nas</span><span class="o">.</span><span class="n">zip</span><span class="o">(</span><span class="n">arr</span><span class="o">).</span><span class="n">foreach</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">n</span><span class="o">,</span> <span class="n">d</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">n</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">d</span><span class="o">)}</span>
        <span class="o">})</span>
        <span class="nc">Iterator</span><span class="o">(</span><span class="n">nas</span><span class="o">)</span>
    <span class="o">})</span>
    <span class="n">nastats</span><span class="o">.</span><span class="n">reduce</span><span class="o">((</span><span class="n">n1</span><span class="o">,</span> <span class="n">n2</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">{</span>
        <span class="n">n1</span><span class="o">.</span><span class="n">zip</span><span class="o">(</span><span class="n">n2</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">merge</span><span class="o">(</span><span class="n">b</span><span class="o">)}</span>
    <span class="o">})</span>
<span class="o">}</span>
</code></pre></div>
<p><br>
<br></p>

<h2>Simple Variable Selection and Scoring</h2>

<p>앞서 작성한 <code>statsWithMissing</code> 함수를 이용해 다음과 같이 parsedRDD 로부터 각 데이터가 매치되는지 여부에 따라 scores의 분포가 어떻게 되는지 구할 수 있다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">val</span> <span class="n">statsm</span> <span class="k">=</span> <span class="n">statsWithMissing</span><span class="o">(</span><span class="n">parsed</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">matched</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">scores</span><span class="o">))</span>
<span class="k">val</span> <span class="n">statsn</span> <span class="k">=</span> <span class="n">statsWithMissing</span><span class="o">(</span><span class="n">parsed</span><span class="o">.</span><span class="n">filter</span><span class="o">(!</span><span class="k">_</span><span class="o">.</span><span class="n">matched</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">scores</span><span class="o">))</span>
</code></pre></div>
<p>두 변수 <code>statsm</code>과 <code>statsn</code>은 전체 데이터를 매치되는지 여부에 따라 두 개의 서브셋으로 나누어 scores의 확률적 분포에 대한 정보를 갖고 있다. 여기서 두 개의 각 서브셋에 존재하는 scores 배열의 각 칼럼 별 정보를 비교함으로써, 두 서브셋 차이에 각 feature 별로 어떤 차이를 갖고 있는지를 비교할 수 있다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">statsm</span><span class="o">.</span><span class="n">zip</span><span class="o">(</span><span class="n">statsn</span><span class="o">).</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span><span class="o">(</span><span class="n">m</span><span class="o">,</span> <span class="n">n</span><span class="o">)</span> <span class="k">=&gt;</span>
    <span class="o">(</span><span class="n">m</span><span class="o">.</span><span class="n">missing</span> <span class="o">+</span> <span class="n">n</span><span class="o">.</span><span class="n">missing</span><span class="o">,</span> <span class="n">m</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">mean</span> <span class="o">-</span> <span class="n">n</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">mean</span><span class="o">)</span>
<span class="o">}.</span><span class="n">zipWithIndex</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre></div>
<p>위 코드의 수행 결과로 다음과 같은 결과가 나온다.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">((1007,0.285452905746686),0)
((5645434,0.09104268062279874),1)
((0,0.6838772482597568),2)
((5746668,0.8064147192926266),3)
((0,0.03240818525033473),4)
((795,0.7754423117834044),5)
((795,0.5109496938298719),6)
((795,0.7762059675300523),7)
((12843,0.9563812499852178),8)
</code></pre></div>
<p>좋은 feature 는 두 가지의 속성이 있다. 하나는 그 feature에 따른 크기가 큰 차이가 존재하는 것이고, 모든 데이터 쌍(두 서브셋 사이의)에 대해서도 균일하게 발생한다는 것이다. 이러한 이론에 따라 인덱스 1의 feature 는 좋은 feature라고 할 수 없다. 값이 존재하지 않아 missing이 카운트 된 횟수가 매우 많으며, 두 서브셋의 평균값의 차이가 0.09로 매우 적다(값의 범위가 0 에서 1인 것임을 감안했을 때). Feature 4 또한 적절치 않다. Feature 4는 모든 값이 존재하지만 평균 값의 차이가 0.03이므로 두 데이터 그룹 사이에 별 차이가 없기 때문이다.</p>

<p>반면, feature 5와 7은 훌륭한 feature 이다. 대부분의 데이터에 대해 값이 존재하며, 두 그룹의 평균 차이가 매우 크기 때문이다. 그리고 feature 2, 6, 8 역시 괜찮은 feature 라 할 수 있다. Feature 0과 3은 좀 애매하다고 할 수 있다. Feature 0은 대부분의 데이터에서 관측 가능하지만 두 셋의 평균 차이가 크지 않고, 반대로 Feature 3은 두 셋의 평균 차이가 크지만 많은 데이터에서 관측하기가 어렵다. 이 정보들은 두 데이터 셋을 명확하게 표현하기가 어렵다.</p>

<p>앞서 설명한 내용을 바탕으로하여 쓸만한 feature(2, 5, 6, 7, 8)를 이용해 각 데이터에 대한 scoring model을 만들 것이다. 이 모델에서는 NaN 값은 0으로 처리하여 계산할 것이다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">def</span> <span class="n">naz</span><span class="o">(</span><span class="n">d</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">=</span> <span class="k">if</span> <span class="o">(</span><span class="nc">Double</span><span class="o">.</span><span class="nc">NaN</span><span class="o">.</span><span class="n">equals</span><span class="o">(</span><span class="n">d</span><span class="o">))</span> <span class="mf">0.0</span> <span class="k">else</span> <span class="n">d</span>
<span class="k">case</span> <span class="k">class</span> <span class="nc">Scored</span><span class="o">(</span><span class="n">md</span><span class="k">:</span> <span class="kt">MatchData</span><span class="o">,</span> <span class="n">score</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span>
<span class="k">val</span> <span class="n">ct</span> <span class="k">=</span> <span class="n">parsed</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">md</span> <span class="k">=&gt;</span> <span class="o">{</span>
    <span class="k">val</span> <span class="n">score</span> <span class="k">=</span> <span class="nc">Array</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="mi">5</span><span class="o">,</span> <span class="mi">6</span><span class="o">,</span> <span class="mi">7</span><span class="o">,</span> <span class="mi">8</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">i</span> <span class="k">=&gt;</span> <span class="n">naz</span><span class="o">(</span><span class="n">md</span><span class="o">.</span><span class="n">scores</span><span class="o">(</span><span class="n">i</span><span class="o">))).</span><span class="n">sum</span>
    <span class="nc">Scored</span><span class="o">(</span><span class="n">md</span><span class="o">,</span> <span class="n">score</span><span class="o">)</span>
<span class="o">})</span>
<span class="n">ct</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">md</span><span class="o">.</span><span class="n">matched</span><span class="o">).</span><span class="n">countByValue</span><span class="o">()</span>

<span class="o">...</span>

<span class="n">scala</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="nc">Map</span><span class="o">[</span><span class="kt">Boolean</span>,<span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Map</span><span class="o">(</span><span class="kc">true</span> <span class="o">-&gt;</span> <span class="mi">20931</span><span class="o">,</span> <span class="kc">false</span> <span class="o">-&gt;</span> <span class="mi">5728201</span><span class="o">)</span>
</code></pre></div>
<p>생성한 <code>ct RDD</code> 에 여러 Threshold 값들을 지정하고, 매치되었는지 여부에 따라 카운팅을 함으로써 데이터의 속성에 대해 파악할 수 있다.</p>

<p>다음 결과는 Threshold 를 4.0 으로 정하였는데, 이것은 각각의 feature 에 대해 평균적으로 0.8 이상을 갖고 있음을 의미한다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">ct</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">score</span> <span class="o">&gt;=</span> <span class="mf">4.0</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">md</span><span class="o">.</span><span class="n">matched</span><span class="o">).</span><span class="n">countByValue</span><span class="o">()</span>

<span class="o">...</span>

<span class="n">scala</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="nc">Map</span><span class="o">[</span><span class="kt">Boolean</span>,<span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Map</span><span class="o">(</span><span class="kc">true</span> <span class="o">-&gt;</span> <span class="mi">20871</span><span class="o">,</span> <span class="kc">false</span> <span class="o">-&gt;</span> <span class="mi">637</span><span class="o">)</span>
</code></pre></div>
<p>위와 같은 결과는 matched에 속하는 데이터 중 true를 갖는 데이터들의 99%이상이 feature(2, 5, 6, 7, 8) 합이 4 이상임을 나타내고, false를 갖는 데이터는 대부분(98.8%)이 4.0 이하의 값을 갖는다는것을 얘기한다.</p>

<p>다음과 같이 score 값을 2.0 으로 필터링 하면 다음과 같은 결과를 얻을 수 있다.</p>
<div class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">ct</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">score</span> <span class="o">&gt;=</span> <span class="mf">2.0</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">.</span><span class="n">md</span><span class="o">.</span><span class="n">matched</span><span class="o">).</span><span class="n">countByValue</span><span class="o">()</span>

<span class="o">...</span>

<span class="n">scala</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="nc">Map</span><span class="o">[</span><span class="kt">Boolean</span>,<span class="kt">Long</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Map</span><span class="o">(</span><span class="kc">true</span> <span class="o">-&gt;</span> <span class="mi">20931</span><span class="o">,</span> <span class="kc">false</span> <span class="o">-&gt;</span> <span class="mi">596414</span><span class="o">)</span>
</code></pre></div>
<p>이 결과는 matched 값이 true 인 경우에는 모든 데이터에가 feature 의 합이 2.0 이상이라는 것과, matched 값이 false인 경우에는 여전이 90% 이상이 feature 합이 2.0 미만이라는 것을 알 수 있다.</p>

<p>앞의 예시에서는 매우 간단한 특정 feature들의 합으로 matched 값에 따라 분류된 두 데이터 셋의 특성에 대해 알아봤지만, 이를 다양하게 변화시킴으로써 주어진 데이터셋에 대해 또 다른 새로운 정보들을 얻을 수 있을 것이다.</p>

<p>지금까지 Spark를 사용해보고, 이를 이용해 간단한 데이터셋을 필터링하고, 데이터셋의 특성에 대해 파악해보았다. 다음 장부터는 또 다른 데이터 셋을 이용해 좀 더 깊이있는 분석을 해 볼 것이다.</p>

  </div><!-- /.entry-content -->
  
  <center><div><a href="/data%20analysis/2015/07/04/advanced-analytics-with-spark-ch2/" class="btn">Read More...</a></div></center>
  
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="/data%20analysis/2015/07/01/advanced-analytics-with-spark-0/" title="Spark & 머신 러닝 - Overview"><img src="/images/bg4.jpg" alt="Spark & 머신 러닝 - Overview"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2015-07-01T13:09:00-04:00"><a href="/data%20analysis/2015/07/01/advanced-analytics-with-spark-0/">July 01, 2015</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Hyunje Jo">Hyunje Jo</a></span></span>
      <!--
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~1 minute
      </span>
      -->
      <!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/data%20analysis/2015/07/01/advanced-analytics-with-spark-0/" rel="bookmark" title="Spark & 머신 러닝 - Overview" itemprop="url">Spark & 머신 러닝 - Overview</a></h1>
    
  </header>
  <div class="entry-content" style="overflow:hidden; height:500px;">
    <p>이 글들은 <a href="http://shop.oreilly.com/product/0636920035091.do">Advanced Analytics with Spark</a>를 공부하면서 정리한 글이다.</p>

<p>Advanced Analytics with Spark는 2015년 4월에 발매된 책으로, Spark 를 이용해서 데이터 분석을 수행하는 방법과 예시에 대해 작성되어 있다.</p>

<p>Spark에 대한 기초적인 내용부터 시작하여 여러 오픈되어 있는 데이터셋을 이용해 데이터 분석을 수행하는 과정이 담겨있다. Spark에서 제공하는 mllib과 graphX 등을 사용하여 분석을 수행한다.</p>

<p>책에서는 Spark 버전은 1.3을 기준으로 하고 있지만, ~~최근 1.4가 공개되었으므로 1.4로 진행할 것이다.~~ 1.5로 진행할 것이다.</p>

<p>진행하는데에 앞서 필요한 사항과 개발 환경은 다음과 같다.
아직 책의 초반을 진행하고 있어서 IDE를 사용할 일이 적은데, 사용하게 된다면 IntelliJ IDEA를 사용할 것이다.</p>

<p><br></p>

<h3>Pre-requirements</h3>

<ul>
<li>Hadoop 2.6+</li>
<li>Spark 1.4</li>
<li>Spark와 Hadoop에 대한 기본적인 지식</li>
</ul>

<h3>Environment</h3>

<ul>
<li>Scala 2.10</li>
<li>Intellij IDEA 14 (사용하게 되면)</li>
</ul>

<h3>Target</h3>

<ul>
<li>제대로 된 Spark의 사용법</li>
<li>데이터 분석 경험</li>
</ul>

<p><br>
<br>
아직까지 내가 겪어본 Spark는 단순하게 예제를 돌려보거나, 혼자의 감에 의지한 주먹구구식의 분석 뿐이었다. 이 책을 따라 공부하면서 제대로 Spark를 사용해 보고 데이터 분석을 경험해 볼 것이다. 그리고 그 경험을 이용해 내가 진행하였던 몇몇 분석 결과와 코드 등을 다시 돌아보고, 수정하여 말끔하게 다듬을 것이다.</p>

<p>또한 진행 도중에 괜찮은 아이디어가 떠오르면 그것을 직접 구현하고 결과를 공개 할 계획이다.</p>

  </div><!-- /.entry-content -->
  
  <center><div><a href="/data%20analysis/2015/07/01/advanced-analytics-with-spark-0/" class="btn">Read More...</a></div></center>
  
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="/framework/2014/11/16/how-to-install-hbase/" title="HBase Pseudo-distributed Mode 설치법"><img src="/images/bg3.jpg" alt="HBase Pseudo-distributed Mode 설치법"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-11-16T04:59:00-05:00"><a href="/framework/2014/11/16/how-to-install-hbase/">November 16, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Hyunje Jo">Hyunje Jo</a></span></span>
      <!--
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~1 minute
      </span>
      -->
      <!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/framework/2014/11/16/how-to-install-hbase/" rel="bookmark" title="HBase Pseudo-distributed Mode 설치법" itemprop="url">HBase Pseudo-distributed Mode 설치법</a></h1>
    
  </header>
  <div class="entry-content" style="overflow:hidden; height:500px;">
    <p>이 문서는 OS X 에서 HBase를 설치하고, Hadoop2 와 연동하는 방법에 대한 글입니다.<br>
Pseudo Distributed Mode로 설치하고 실행하는 과정입니다.<br>
Java Client API 를 이용하여 간단한 테스트까지 수행합니다.</p>

<h3>WARNING</h3>

<p>이 글은 최신버전을 기준으로 설명된 글이 아닙니다. 최신 버전을 대상으로 하였을 때와 설치 과정 혹은 출력 결과가 다를 수 있습니다.</p>

<p><br></p>

<h3>Install Environments</h3>

<p>설치 과정에서 사용된 여러 환경에 관한 내용입니다.</p>

<ul>
<li>Hadoop 2.5.1</li>
<li>Maven 3</li>
<li>IntelliJ IDEA</li>
</ul>

<h3>Procedure</h3>

<p>설치 과정에 대한 요약입니다. 5번과 6번은 다음 포스트에서 이어서 다루도록 하겠습니다.</p>

<ol>
<li>hosts 수정</li>
<li>Zookeeper 설치 &amp; 설정 </li>
<li>HBase 설치 &amp; 설정</li>
<li>HBase 실행</li>
<li>간단한 쿼리를 이용한 설치 테스트</li>
<li>Client API 사용한 테스트</li>
</ol>

<h4>1. /etc/hosts 수정</h4>

<p>Hbase에서 peudo-distributed 모드를 정상적으로 사용하기 위해서는 /etc/hosts를 수정해주어야 합니다.<br></p>
<div class="highlight"><pre><code class="language-text" data-lang="text">127.0.0.1   localhost
</code></pre></div>
<p>부분을 삭제 혹은 주석처리 후, 다음 예시와 같이 실제 IP를 localhost 로 지정해주어야 합니다.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">a.b.c.d     localhost
</code></pre></div>
<h4>2. Zookeeper 설치 &amp; 설정</h4>

<p><a href="http://mirror.apache-kr.org/zookeeper/">다음 페이지</a>에서 Zookeeper를 다운로드합니다.<br>
저는 Stable 버전인 3.4.6 버전을 다운로드 하였습니다.<br>
압축 해제한 경로를 <code>${ZK_HOME}</code>으로 정의합니다.
<code>${ZK_HOME}/conf</code>폴더로 이동하여 다음 명령어로 설정 파일을 생성합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cp zoo_sample.cfg zoo.cfg
</code></pre></div>
<p>새로 생성한 파일 <code>zoo.cfg</code>에서 <code>dataDir</code>변수를 Zookeeper 데이터를 저장 할 경로를 지정해줍니다. 다른 설정값은 변경하지 않아도 무방합니다.<br>
다음 명령어를 이용하여 정상적으로 Zookeeper Server가 수행되는지 확인합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="k">${</span><span class="nv">ZK_HOME</span><span class="k">}</span>/bin/zkServer.sh start
<span class="nv">$ </span><span class="k">${</span><span class="nv">ZK_HOME</span><span class="k">}</span>/bin/zkCli.sh -server IP_Address:2181
</code></pre></div>
<p><code>quit</code>명렁어로 클라이언트를 종료한 후, 다음 명령어로 다시 Zookeeper 서버를 종료합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="k">${</span><span class="nv">ZK_HOME</span><span class="k">}</span>/bin/zkServer.sh stop
</code></pre></div>
<h4>3. HBase 설치 &amp; 설정</h4>

<p>이 과정부터는 Hadoop 2.x 가 켜져 있는 상태임을 가정합니다. Hadoop이 켜져 있지 않은 상황이라면 Hadoop을 켜 주시기 바랍니다.<br>
<a href="http://mirror.apache-kr.org/hbase/stable/">다음 페이지</a>에서 HBase를 다운로드합니다.<br>
다운로드 할 때는, Hadoop의 버전과 맞는 접미사를 다운로드해야 합니다. 이 문서에서는 Hadoop 2.5.1을 기반으로 하기 때문에 <code>hbase-0.98.7-hadoop2-bin.tar.gz</code>를 다운로드 하였습니다.<br>
다운로드한 압축파일을 해제하고, 그 경로를 <code>${HBASE_HOME}</code>으로 정의합니다.<br>
HBase에서 수정해야 할 설정파일은 <code>hbase-site.xml</code>과 <code>hbase-env.sh</code> 입니다.</p>

<h5>hbase-site.xml</h5>

<p><code>${HBASE_HOME}/conf/hbase-site.xml</code>파일을 다음과 같이 설정합니다.</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.rootdir<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>hdfs://IP_ADDRESS:9000/hbase<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.zookeeper.quorum<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>IP_ADDRESS<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.zookeeper.property.dataDir<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>Zookeeper의 DataDir<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.cluster.distributed<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.master.info.port<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>60010<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.master.info.bindAddress<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>IP_ADDRESS<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>dfs.support.append<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>dfs.datanode.max.xcievers<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>4096<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.zookeeper.property.clientPort<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>2181<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.regionserver.info.bindAddress<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>IP_ADDRESS<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div>
<h5>hbase-env.sh</h5>

<p><code>${HBASE_HOME}/conf/hbase-env.sh</code>파일의 몇몇 변수를 다음과 같이 설정합니다.(주석 해제 후 값을 변경해 주면 됩니다.) <br></p>

<ul>
<li><p>JAVA_HOME</p>

<p>시스템에 설치되어 있는 JDK의 경로를 지정해줍니다. JDK6 or JDK7을 추천합니다. <a href="http://hbase.apache.org/book/configuration.html#java">참고</a></p></li>
<li><p>HBASE<em>MANAGES</em>ZK</p>

<p>true로 설정합니다.</p></li>
</ul>

<p>다음은 설정의 예시입니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/usr/lib/jvm/java-7-openjdk-amd64
<span class="nb">export </span><span class="nv">HBASE_MANAGES_ZK</span><span class="o">=</span><span class="nb">true</span>
</code></pre></div>
<p>위 두 항목이 주석 해제되어 있고, 정상적인 값으로 설정되어있어야 합니다.</p>

<h4>4. Hbase 실행</h4>

<p>다음 명령어를 이용하여 HBase를 실행합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="k">${</span><span class="nv">HBASE_HOME</span><span class="k">}</span>/bin/start-hbase.sh
</code></pre></div>
<p>다음 명령어를 이용하여 HDFS에 HBase 폴더가 정상적으로 생성되었는지 확인합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="k">${</span><span class="nv">HADOOP_HOME</span><span class="k">}</span>/bin/hdfs dfs -ls /hbase
</code></pre></div>
<p>그리고 다음 명령어를 이용하여 Zookeeper에도 Hbase가 정상적으로 등록되었는지 확인합니다.<br>
(<code>${HADOOP_HOME}</code>은 Hadoop이 설치되어 있는 경로입니다.)</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">${ZK_HOME}/bin/zkCli.sh -server IP_ADDRESS:2181
...
[zk:IP_ADDRESS:2181 (CONNECTED)] ls /
</code></pre></div>
<p>출력되는 항목에 <code>hbase</code>가 있어야 합니다.</p>

  </div><!-- /.entry-content -->
  
  <center><div><a href="/framework/2014/11/16/how-to-install-hbase/" class="btn">Read More...</a></div></center>
  
</article><!-- /.hentry -->



<div class="pagination">
  <ul class="inline-list">
    
    
      
        <li><a href="/page2/" class="btn">Previous</a></li>
      
    

    
    
      <li><a href="">1</a></li>
    

    
    

    
    
    

    
      
        
        
        
        <li><a href="/page2/">2</a></li>
      
    
      
        <li><strong class="current-page">3</strong></li>
      
    
      
        
        
        
          
          
        
        <li><a href="/page4/">4</a></li>
      
    

    
    

    
      <li><a href="/page5/">5</a></li>
    

    
    
      <li><a href="/page4/" class="btn">Next</a></li>
    
  </ul>
</div>

</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2016 Hyunje Jo.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="/assets/js/scripts.min.js"></script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-62281776-2', 'auto');
  ga('send', 'pageview');

</script>


<!-- Mathjax -->
<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


          

</body>
</html>