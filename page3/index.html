<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Latest Posts &#8211; Hyunje Blog</title>
<meta name="description" content="Describe this nonsense.">
<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Latest Posts">
<meta property="og:description" content="Describe this nonsense.">
<meta property="og:url" content="/page3/index.html">
<meta property="og:site_name" content="Hyunje Blog">





<link rel="canonical" href="/page3/">
<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Hyunje Blog Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">
<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
<link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
<link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/images/apple-touch-icon-144x144-precomposed.png">




<style type="text/css">body {background-image:url(/images/white.jpg);}</style>


</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="/images/avatar.jpg" alt="Hyunje Jo photo" class="author-photo">
					<h4>Hyunje Jo</h4>
					<p>Bigdata-technology based Machine Learning & Data Analysis</p>
				</li>
				<li><a href="/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:retriever89@gmail.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				<li>
					<a href="https://facebook.com/RetrieverJo"><i class="fa fa-fw fa-facebook"></i> Facebook</a>
				</li>
				
				
				<li>
					<a href="https://github.com/retrieverJo"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="/posts/">All Posts</a></li>
				<li><a href="/categories/">All Categories</a></li>
				<li><a href="/tags/">All Tags</a></li>
			</ul>
		</li>
		<!--
		
	    
	        
	    
	    <li><a href="/theme-setup/" >Theme Setup</a></li>
	  
	    
	        
	        
	    <li><a href="http://mademistakes.com" target="_blank">External Link</a></li>
	  
	  	-->
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="">Hyunje Jo</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="/images/bg0.jpg" alt="Latest Posts">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Hyunje Blog</h1>
      <h2>Latest Posts</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="/framework/2014/11/16/how-to-install-hbase/" title="HBase Pseudo-distributed Mode 설치법"><img src="/images/bg3.jpg" alt="HBase Pseudo-distributed Mode 설치법"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-11-16T04:59:00-05:00"><a href="/framework/2014/11/16/how-to-install-hbase/">November 16, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Hyunje Jo">Hyunje Jo</a></span></span>
      <!--
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~1 minute
      </span>
      -->
      <!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/framework/2014/11/16/how-to-install-hbase/" rel="bookmark" title="HBase Pseudo-distributed Mode 설치법" itemprop="url">HBase Pseudo-distributed Mode 설치법</a></h1>
    
  </header>
  <div class="entry-content" style="overflow:hidden; height:500px;">
    <p>이 문서는 OS X 에서 HBase를 설치하고, Hadoop2 와 연동하는 방법에 대한 글입니다.<br>
Pseudo Distributed Mode로 설치하고 실행하는 과정입니다.<br>
Java Client API 를 이용하여 간단한 테스트까지 수행합니다.</p>

<h3>WARNING</h3>

<p>이 글은 최신버전을 기준으로 설명된 글이 아닙니다. 최신 버전을 대상으로 하였을 때와 설치 과정 혹은 출력 결과가 다를 수 있습니다.</p>

<p><br></p>

<h3>Install Environments</h3>

<p>설치 과정에서 사용된 여러 환경에 관한 내용입니다.</p>

<ul>
<li>Hadoop 2.5.1</li>
<li>Maven 3</li>
<li>IntelliJ IDEA</li>
</ul>

<h3>Procedure</h3>

<p>설치 과정에 대한 요약입니다. 5번과 6번은 다음 포스트에서 이어서 다루도록 하겠습니다.</p>

<ol>
<li>hosts 수정</li>
<li>Zookeeper 설치 &amp; 설정 </li>
<li>HBase 설치 &amp; 설정</li>
<li>HBase 실행</li>
<li>간단한 쿼리를 이용한 설치 테스트</li>
<li>Client API 사용한 테스트</li>
</ol>

<h4>1. /etc/hosts 수정</h4>

<p>Hbase에서 peudo-distributed 모드를 정상적으로 사용하기 위해서는 /etc/hosts를 수정해주어야 합니다.<br></p>
<div class="highlight"><pre><code class="language-text" data-lang="text">127.0.0.1   localhost
</code></pre></div>
<p>부분을 삭제 혹은 주석처리 후, 다음 예시와 같이 실제 IP를 localhost 로 지정해주어야 합니다.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">a.b.c.d     localhost
</code></pre></div>
<h4>2. Zookeeper 설치 &amp; 설정</h4>

<p><a href="http://mirror.apache-kr.org/zookeeper/">다음 페이지</a>에서 Zookeeper를 다운로드합니다.<br>
저는 Stable 버전인 3.4.6 버전을 다운로드 하였습니다.<br>
압축 해제한 경로를 <code>${ZK_HOME}</code>으로 정의합니다.
<code>${ZK_HOME}/conf</code>폴더로 이동하여 다음 명령어로 설정 파일을 생성합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>cp zoo_sample.cfg zoo.cfg
</code></pre></div>
<p>새로 생성한 파일 <code>zoo.cfg</code>에서 <code>dataDir</code>변수를 Zookeeper 데이터를 저장 할 경로를 지정해줍니다. 다른 설정값은 변경하지 않아도 무방합니다.<br>
다음 명령어를 이용하여 정상적으로 Zookeeper Server가 수행되는지 확인합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="k">${</span><span class="nv">ZK_HOME</span><span class="k">}</span>/bin/zkServer.sh start
<span class="nv">$ </span><span class="k">${</span><span class="nv">ZK_HOME</span><span class="k">}</span>/bin/zkCli.sh -server IP_Address:2181
</code></pre></div>
<p><code>quit</code>명렁어로 클라이언트를 종료한 후, 다음 명령어로 다시 Zookeeper 서버를 종료합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="k">${</span><span class="nv">ZK_HOME</span><span class="k">}</span>/bin/zkServer.sh stop
</code></pre></div>
<h4>3. HBase 설치 &amp; 설정</h4>

<p>이 과정부터는 Hadoop 2.x 가 켜져 있는 상태임을 가정합니다. Hadoop이 켜져 있지 않은 상황이라면 Hadoop을 켜 주시기 바랍니다.<br>
<a href="http://mirror.apache-kr.org/hbase/stable/">다음 페이지</a>에서 HBase를 다운로드합니다.<br>
다운로드 할 때는, Hadoop의 버전과 맞는 접미사를 다운로드해야 합니다. 이 문서에서는 Hadoop 2.5.1을 기반으로 하기 때문에 <code>hbase-0.98.7-hadoop2-bin.tar.gz</code>를 다운로드 하였습니다.<br>
다운로드한 압축파일을 해제하고, 그 경로를 <code>${HBASE_HOME}</code>으로 정의합니다.<br>
HBase에서 수정해야 할 설정파일은 <code>hbase-site.xml</code>과 <code>hbase-env.sh</code> 입니다.</p>

<h5>hbase-site.xml</h5>

<p><code>${HBASE_HOME}/conf/hbase-site.xml</code>파일을 다음과 같이 설정합니다.</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;configuration&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.rootdir<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>hdfs://IP_ADDRESS:9000/hbase<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.zookeeper.quorum<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>IP_ADDRESS<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.zookeeper.property.dataDir<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>Zookeeper의 DataDir<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.cluster.distributed<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.master.info.port<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>60010<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.master.info.bindAddress<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>IP_ADDRESS<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>dfs.support.append<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>dfs.datanode.max.xcievers<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>4096<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.zookeeper.property.clientPort<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>2181<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
        <span class="nt">&lt;property&gt;</span>
                <span class="nt">&lt;name&gt;</span>hbase.regionserver.info.bindAddress<span class="nt">&lt;/name&gt;</span>
                <span class="nt">&lt;value&gt;</span>IP_ADDRESS<span class="nt">&lt;/value&gt;</span>
        <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</code></pre></div>
<h5>hbase-env.sh</h5>

<p><code>${HBASE_HOME}/conf/hbase-env.sh</code>파일의 몇몇 변수를 다음과 같이 설정합니다.(주석 해제 후 값을 변경해 주면 됩니다.) <br></p>

<ul>
<li><p>JAVA_HOME</p>

<p>시스템에 설치되어 있는 JDK의 경로를 지정해줍니다. JDK6 or JDK7을 추천합니다. <a href="http://hbase.apache.org/book/configuration.html#java">참고</a></p></li>
<li><p>HBASE<em>MANAGES</em>ZK</p>

<p>true로 설정합니다.</p></li>
</ul>

<p>다음은 설정의 예시입니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/usr/lib/jvm/java-7-openjdk-amd64
<span class="nb">export </span><span class="nv">HBASE_MANAGES_ZK</span><span class="o">=</span><span class="nb">true</span>
</code></pre></div>
<p>위 두 항목이 주석 해제되어 있고, 정상적인 값으로 설정되어있어야 합니다.</p>

<h4>4. Hbase 실행</h4>

<p>다음 명령어를 이용하여 HBase를 실행합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="k">${</span><span class="nv">HBASE_HOME</span><span class="k">}</span>/bin/start-hbase.sh
</code></pre></div>
<p>다음 명령어를 이용하여 HDFS에 HBase 폴더가 정상적으로 생성되었는지 확인합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span><span class="k">${</span><span class="nv">HADOOP_HOME</span><span class="k">}</span>/bin/hdfs dfs -ls /hbase
</code></pre></div>
<p>그리고 다음 명령어를 이용하여 Zookeeper에도 Hbase가 정상적으로 등록되었는지 확인합니다.<br>
(<code>${HADOOP_HOME}</code>은 Hadoop이 설치되어 있는 경로입니다.)</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">${ZK_HOME}/bin/zkCli.sh -server IP_ADDRESS:2181
...
[zk:IP_ADDRESS:2181 (CONNECTED)] ls /
</code></pre></div>
<p>출력되는 항목에 <code>hbase</code>가 있어야 합니다.</p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="/data%20analysis/2014/11/06/spark-rdd-functions/" title="Spark RDD의 함수 동작 방식"><img src="/images/bg2.jpg" alt="Spark RDD의 함수 동작 방식"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-11-06T09:09:00-05:00"><a href="/data%20analysis/2014/11/06/spark-rdd-functions/">November 06, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Hyunje Jo">Hyunje Jo</a></span></span>
      <!--
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~1 minute
      </span>
      -->
      <!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/data%20analysis/2014/11/06/spark-rdd-functions/" rel="bookmark" title="Spark RDD의 함수 동작 방식" itemprop="url">Spark RDD의 함수 동작 방식</a></h1>
    
  </header>
  <div class="entry-content" style="overflow:hidden; height:500px;">
    <p>이 글은 Spark의 RDD에 존재하는 함수들이 어떤 방식으로 동작되는지에 대한 글입니다. <br>
<a href="http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds">Spark 의 공식 Documentation</a>을 일부 번역하였습니다.<br>
또한, Java 기준으로 설명 할 것이며, Scala 로 된 버전은 추후에 추가 작성할 계획입니다.<br>
번역하기에 적절치 않은 용어들은 영문 단어 그대로 남겨놓았습니다.
<hr>
<br></p>

<h3>WARNING</h3>

<p>이 글은 최신버전을 기준으로 설명된 글이 아닙니다. 최신 버전을 대상으로 하였을 때와 설치 과정 혹은 출력 결과가 다를 수 있습니다.</p>

<p><br></p>

<h3>Spark의 RDD</h3>

<p>Spark는 Resilient Distributed Dataset, RDD 로 구성되어 있습니다. 이 RDD는 분산 형태로 처리 가능한 fault-tolerant collection 입니다. Spark에서 RDD를 생성하는 방법은 두 가지가 있습니다. 첫 번째로는 Driver 프로그램에서 이미 존재하는 colleciton을 <em>parallelizing</em> 시키는 방법이 있고, 다른 방법으로는 HDFS나 HBase혹은 Hadoop InputFormat 으로 수행 가능한 어떠한 데이터 소스를 <em>referencing</em>하는 방법이 있습니다.</p>

<p><br></p>

<h5>Parallelized Collections</h5>

<p>Java에서 Parallelized Collection을 생성하기 위해서는 <code>JavaSparkContext</code>클래스에 존재하는 <code>parallelize</code> 함수에 Driver 프로그램에서 사용한 Collection을 파라미터로 넘겨주면 됩니다. Collection에 존재하는 엘리먼트들은 Distributed Dataset으로 복사되고, 분산 형태로 연산됩니다. 1 에서 5 까지 값을 갖는 리스트를 parallelized collection으로 생성하는 방법은 다음 예와 같습니다.</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">List</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="o">,</span> <span class="mi">3</span><span class="o">,</span> <span class="mi">4</span><span class="o">,</span> <span class="mi">5</span><span class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">&gt;</span> <span class="n">distData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">parallelize</span><span class="o">(</span><span class="n">data</span><span class="o">);</span>
</code></pre></div>
<p>한번 생성된 distributed dataset(<code>distData</code>)는 분산 형태로 연산될 수 있습니다. 예를들면, 다음과 같은 코드를 이용하여 리스트에 존재하는 모든 값을 더할 수 있습니다.</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">distData</span><span class="o">.</span><span class="na">reduce</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">);</span>
</code></pre></div>
<p>이러한 분산 형태의 연산에 대해서는 아래 Section 에서 설명할 것입니다.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">이 Documentation에서는 Java 8 에서 제공하는 Lamda 문법을 사용하고 있습니다. Java 8을 사용할 수 없는 상황이어서 lambda 표현식을 사용하지 못할 때는, org.apache.spark.api.java.function 패키지를 구현하여 사용할 수 있습니다. 이에 대해서는 아래 Section에서 설명할 것입니다.
</code></pre></div>
<p>parallel collection에서 중요한 파라메터중 하나는 데이터셋을 몇 개의 <code>slices</code>로 나눌 것인지에 대한 것입니다. Spark는 각 cluster의 조각마다 하나의 태스크를 수행 하게 됩니다. 일반적으로 클러스터에서 각각의 CPU 마다 2 ~ 4 개의 slice를 합니다. Spark는 클러스터를 기반으로 slice의 수를 자동적으로 생성을 시도하지만, <code>parallelize</code>를 수행할 때 그 개수를 수동으로 지정해 줄 수 있습니다. (e.g. <code>sc.parallelize(data, 10)</code>)</p>

<p><br></p>

<h4>External Datasets</h4>

<p>Spark에서는 Hadoop과 연관되는 모든 데이터 소스(Local File System, HDFS, Cassandra, HBase, Amazon S3, etc.)를 이용하여 distributed dataset을 생성할 수 있습니다. Spark는 텍스트파일, <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">Sequence File</a>과 모든 Hadoop <a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/InputFormat.html">InputFormat</a>을 지원합니다.</p>

<p>텍스트 파일의 RDD는 <code>SparkContext</code>의 <code>textFile</code> 함수를 사용하여 생성될 수 있으며, 이 함수는 파일의 URI를 이용하여 파일에 접근합니다. 그리고 텍스트 파일의 한 라인의 collection으로 읽고, 다음과 같은 형태로 파일을 불러옵니다.</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">distFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">&quot;data.txt&quot;</span><span class="o">);</span>
</code></pre></div>
<p>한번 생성이 되면, <code>distFile</code>은 dataset 연산들을 수행할 수 있게 됩니다. 예를들면 <code>map</code>과 <code>reduce</code>를 이용하여 모든 라인의 길이를 더한 갚을 구할때는 다음과 같이 명령어를 수행하면 됩니다.</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">distFile</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="n">s</span><span class="o">.</span><span class="na">length</span><span class="o">()).</span><span class="na">reduce</span><span class="o">(</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">)</span>
</code></pre></div>
<p>Spark를 이용하여 파일을 읽을 때, 다음 사항들을 참고할 수 있습니다.</p>

<ul>
<li>Local File System 에 있는 파일에 접근할 때, 반드시 Worker node에서 접근 가능한 경로에 파일이 있어야 한다. 모든 Worker에게 파일을 복사하거나, network-mounted 로 공유된 파일 시스템을 사용해야 합니다.<br><br></li>
<li>Spark의 file-based 입력 함수는 폴더, 압축파일과 와일드카드 표현을 포함하여 사용될 수 있습니다. 예를 들면 <code>textFile(&quot;/my/directory&quot;)</code>, <code>textFile(&quot;/my/directory/*.txt&quot;)</code>, <code>textFile(&quot;/my/directory/*/gz&quot;)</code>가 모두 가능합니다.<br><br></li>
</ul>

<p>Spark는 텍스트 파일 이외에도 많은 데이터 포맷을 제공합니다.</p>

<ul>
<li><code>JavaSaprkContext.wholeTextFiles</code>는 하나의 폴더 안에 존재하는 텍스트 파일을 읽습니다. 그리고 그것을 &lt;파일이름, 내용&gt; 형태의 쌍으로 리턴합니다. 이것은 한 파일을 읽어 각각의 줄에 대해 처리하는 <code>textFile</code>과는 다른 형태입니다.<br><br></li>
<li><a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/mapred/SequenceFileInputFormat.html">SequenceFiles</a>를 읽기 위해서는 SparkContext의 <code>sequenceFile[K,V]</code> 함수를 사용해야합니다. K 와 V는 해당 파일에서 사용하고 있는 Key와 Value의 타입입니다. 이것들은 Hadoop의 IntWritable 과 Text 클래스와 같이 <a href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/Writable.html">Writable</a>클래스의 subclass여야 합니다. Spark에서는 이러한 과정을 돕기 위해 몇 개의 일반적인 Writable 타입에 대해 native type을 제공합니다. 예를들어 <code>sequenceFile[Int,String]</code>을 사용하면, 이것은 <code>IntWritable</code>과 <code>Text</code>로 인식됩니다.<br><br></li>
<li>다른 Hadoop InputFormat을 사용하기 위해서는 <code>JavaSparkContext.HadoopRDD</code>함수를 사용해야 합니다. 이 함수는 임의의 JobConf와 InputFormat 클래스, Key 클래스, Value 클래스를 받습니다. Hadoop 에서 입력 소스를 지정하는 과정과 같은 방식으로 지정을 해야 합니다. 또한 새 맵리듀스 API(org.apache.hadoop.mapreduce 패키지에 존재하는 API)를 사용하기 위해서는<code>JavaSparkContext.newHadoopRDD</code>를 사용해야합니다.<br><br></li>
<li><code>JavaRDD.saveAsObjectFile</code>과 <code>JavaSparkContext.objectFile</code>은 Serialized된 자바 객체 형태로 출력하는 것을 지원합니다. 이것은 Avro와 같이 특수화 된 형태보다는 효율적이지 않지만 간단한 형태로 어떠한 RDD도 저장 가능합니다.</li>
</ul>

<p><br></p>

<h4>RDD Operations</h4>

<p>추후 번역</p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="/data%20analysis/2014/11/05/recommendation-with-spark/" title="Spark 기반의 추천 알고리즘 수행"><img src="/images/bg1.jpg" alt="Spark 기반의 추천 알고리즘 수행"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-11-05T06:48:00-05:00"><a href="/data%20analysis/2014/11/05/recommendation-with-spark/">November 05, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="/about/" title="About Hyunje Jo">Hyunje Jo</a></span></span>
      <!--
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~1 minute
      </span>
      -->
      <!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="/data%20analysis/2014/11/05/recommendation-with-spark/" rel="bookmark" title="Spark 기반의 추천 알고리즘 수행" itemprop="url">Spark 기반의 추천 알고리즘 수행</a></h1>
    
  </header>
  <div class="entry-content" style="overflow:hidden; height:500px;">
    <p>이 글은 Spark를 기반으로 하여 추천 알고리즘을 수행하는 과정에 대한 것입니다.<br>
추천에 사용되는 데이터셋은 <a href="http://grouplens.org/datasets/movielens/">MovieLens</a>를 기준으로 하였습니다.</p>

<h3>WARNING</h3>

<p>이 글은 최신버전을 기준으로 설명된 글이 아닙니다. 최신 버전을 대상으로 하였을 때와 설치 과정 혹은 출력 결과가 다를 수 있습니다.</p>

<p><br></p>

<h4>Environment</h4>

<ul>
<li>Hadoop 2.5.1 - <a href="http://hyunje.com/post/os-xe-hadoop2-dot-5-1-seolcihagi/">설치과정</a></li>
<li>Spark 1.1.0 - <a href="http://hyunje.com/post/spark-1-dot-1-0-seolci,-hadoop-2-dot-5-.1gwayi-yeondong/">설치과정</a></li>
<li>IntelliJ IDEA</li>
<li>Maven </li>
<li>JDK 1.7</li>
</ul>

<p><br></p>

<h4>Dependency</h4>

<p>Spark의 sub-project인 MLlib 프로젝트에서 이미 추천 알고리즘에 대한 라이브러리를 구현해 놓았습니다. 이를 사용하기 위해서 pom.xml에 다음과 같은 dependency를 추가합니다.</p>
<div class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.spark<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>spark-core_2.10<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>1.1.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.spark<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>spark-mllib_2.10<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>1.1.0<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</code></pre></div>
<p><br></p>

<h4>Recommendation Module</h4>

<p>다음과 같은 형태로 추천 알고리즘을 사용하고 그 결과를 저장합니다.<br>
JDK 1.7을 사용하였기 때문에 람다표현을 사용하지 않았습니다. Java 8을 사용하면 람다표현을 사용할 수 있으며, 좀 더 간략하게 Spark 프로그램을 작성할 수 있습니다.</p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">SparkConf</span><span class="o">().</span><span class="na">setAppName</span><span class="o">(</span><span class="s">&quot;Spark-recommendation&quot;</span><span class="o">).</span><span class="na">setMaster</span><span class="o">(</span><span class="s">&quot;yarn-cluster&quot;</span><span class="o">);</span>
<span class="n">JavaSparkContext</span> <span class="n">context</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">JavaSparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span>

<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">data</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="n">INPUT_PATH</span><span class="o">);</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Rating</span><span class="o">&gt;</span> <span class="n">ratings</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="na">map</span><span class="o">(</span>
        <span class="k">new</span> <span class="n">Function</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Rating</span><span class="o">&gt;()</span> <span class="o">{</span>
            <span class="kd">public</span> <span class="n">Rating</span> <span class="nf">call</span><span class="o">(</span><span class="n">String</span> <span class="n">s</span><span class="o">)</span> <span class="o">{</span>
                <span class="n">String</span><span class="o">[]</span> <span class="n">sarray</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="n">delimiter</span><span class="o">);</span>
                <span class="k">return</span> <span class="k">new</span> <span class="nf">Rating</span><span class="o">(</span><span class="n">Integer</span><span class="o">.</span><span class="na">parseInt</span><span class="o">(</span><span class="n">sarray</span><span class="o">[</span><span class="mi">0</span><span class="o">]),</span> <span class="n">Integer</span><span class="o">.</span><span class="na">parseInt</span><span class="o">(</span><span class="n">sarray</span><span class="o">[</span><span class="mi">1</span><span class="o">]),</span>
                        <span class="n">Double</span><span class="o">.</span><span class="na">parseDouble</span><span class="o">(</span><span class="n">sarray</span><span class="o">[</span><span class="mi">2</span><span class="o">]));</span>
            <span class="o">}</span>
        <span class="o">}</span>
<span class="o">);</span>

<span class="c1">// Build the recommendation model using ALS</span>
<span class="kt">int</span> <span class="n">rank</span> <span class="o">=</span> <span class="mi">10</span><span class="o">;</span>
<span class="kt">int</span> <span class="n">numIterations</span> <span class="o">=</span> <span class="mi">20</span><span class="o">;</span>
<span class="n">MatrixFactorizationModel</span> <span class="n">model</span> <span class="o">=</span> <span class="n">ALS</span><span class="o">.</span><span class="na">train</span><span class="o">(</span><span class="n">JavaRDD</span><span class="o">.</span><span class="na">toRDD</span><span class="o">(</span><span class="n">ratings</span><span class="o">),</span> <span class="n">rank</span><span class="o">,</span> <span class="n">numIterations</span><span class="o">,</span> <span class="mf">0.01</span><span class="o">);</span>

<span class="c1">// Evaluate the model on rating data</span>
<span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Object</span><span class="o">,</span> <span class="n">Object</span><span class="o">&gt;&gt;</span> <span class="n">userProducts</span> <span class="o">=</span> <span class="n">ratings</span><span class="o">.</span><span class="na">map</span><span class="o">(</span>
        <span class="k">new</span> <span class="n">Function</span><span class="o">&lt;</span><span class="n">Rating</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Object</span><span class="o">,</span> <span class="n">Object</span><span class="o">&gt;&gt;()</span> <span class="o">{</span>
            <span class="kd">public</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Object</span><span class="o">,</span> <span class="n">Object</span><span class="o">&gt;</span> <span class="nf">call</span><span class="o">(</span><span class="n">Rating</span> <span class="n">r</span><span class="o">)</span> <span class="o">{</span>
                <span class="k">return</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Object</span><span class="o">,</span> <span class="n">Object</span><span class="o">&gt;(</span><span class="n">r</span><span class="o">.</span><span class="na">user</span><span class="o">(),</span> <span class="n">r</span><span class="o">.</span><span class="na">product</span><span class="o">());</span>
            <span class="o">}</span>
        <span class="o">}</span>
<span class="o">);</span>
<span class="n">JavaPairRDD</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;,</span> <span class="n">Double</span><span class="o">&gt;</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">JavaPairRDD</span><span class="o">.</span><span class="na">fromJavaRDD</span><span class="o">(</span>
        <span class="n">model</span><span class="o">.</span><span class="na">predict</span><span class="o">(</span><span class="n">JavaRDD</span><span class="o">.</span><span class="na">toRDD</span><span class="o">(</span><span class="n">userProducts</span><span class="o">)).</span><span class="na">toJavaRDD</span><span class="o">().</span><span class="na">map</span><span class="o">(</span>
                <span class="k">new</span> <span class="n">Function</span><span class="o">&lt;</span><span class="n">Rating</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;,</span> <span class="n">Double</span><span class="o">&gt;&gt;()</span> <span class="o">{</span>
                    <span class="kd">public</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;,</span> <span class="n">Double</span><span class="o">&gt;</span> <span class="nf">call</span><span class="o">(</span><span class="n">Rating</span> <span class="n">r</span><span class="o">)</span> <span class="o">{</span>
                        <span class="k">return</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;,</span> <span class="n">Double</span><span class="o">&gt;(</span>
                                <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;(</span><span class="n">r</span><span class="o">.</span><span class="na">user</span><span class="o">(),</span> <span class="n">r</span><span class="o">.</span><span class="na">product</span><span class="o">()),</span> <span class="n">r</span><span class="o">.</span><span class="na">rating</span><span class="o">());</span>
                    <span class="o">}</span>
                <span class="o">}</span>
        <span class="o">));</span>

<span class="c1">//&lt;&lt;Integer,Integer&gt;,Double&gt; to &lt;Integer,&lt;Integer,Double&gt;&gt;</span>
<span class="n">JavaPairRDD</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;&gt;</span> <span class="n">userPredictions</span> <span class="o">=</span> <span class="n">JavaPairRDD</span><span class="o">.</span><span class="na">fromJavaRDD</span><span class="o">(</span><span class="n">predictions</span><span class="o">.</span><span class="na">map</span><span class="o">(</span>
        <span class="k">new</span> <span class="n">Function</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;,</span> <span class="n">Double</span><span class="o">&gt;,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;&gt;&gt;()</span> <span class="o">{</span>
            <span class="nd">@Override</span>
            <span class="kd">public</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;&gt;</span> <span class="nf">call</span><span class="o">(</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;,</span> <span class="n">Double</span><span class="o">&gt;</span> <span class="n">v1</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
                <span class="k">return</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;&gt;(</span><span class="n">v1</span><span class="o">.</span><span class="na">_1</span><span class="o">().</span><span class="na">_1</span><span class="o">(),</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Integer</span><span class="o">,</span> <span class="n">Double</span><span class="o">&gt;(</span><span class="n">v1</span><span class="o">.</span><span class="na">_1</span><span class="o">().</span><span class="na">_2</span><span class="o">(),</span> <span class="n">v1</span><span class="o">.</span><span class="na">_2</span><span class="o">()));</span>
            <span class="o">}</span>
        <span class="o">}</span>
<span class="o">));</span>

<span class="c1">//Sort by key &amp; Save</span>
<span class="n">userPredictions</span><span class="o">.</span><span class="na">sortByKey</span><span class="o">(</span><span class="kc">true</span><span class="o">).</span><span class="na">saveAsTextFile</span><span class="o">(</span><span class="n">OUTPUT_PATH</span><span class="o">);</span>
<span class="n">context</span><span class="o">.</span><span class="na">stop</span><span class="o">();</span>
</code></pre></div>
<p>전체 소스코드는 <a href="https://github.com/RetrieverJo/Spark-Example">github repository</a>에 있습니다.</p>

<p><br></p>

<h4>Preparation</h4>

<p>Input file은 <a href="https://github.com/apache/spark">Spark repository</a>의 <a href="https://github.com/apache/spark/blob/master/data/mllib/sample_movielens_data.txt">ALS 샘플 데이터</a>를 이용하였습니다.<br>
해당 파일을 HDFS에 업로드 한 후,  Input File 로 사용합니다.<br>
위 코드에서는 INPUT_PATH로 정의되었지만, github repository 에서는 Apache Commons-cli 를 이용하여 입력받았습니다.
<br>
<br></p>

<h4>Run Spark Application</h4>

<p>maven을 이용하여 프로젝트를 패키징 합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>mvn package
</code></pre></div>
<p>다음과 같은 명령어를 이용하여 작성한 Wordcount application을 YARN에 submit 합니다.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>spark-submit --class 패키지명.클래스명 --master yarn-cluster Package된Jar파일.jar
</code></pre></div>
<p>(github repository에는 Apache Commons-cli 를 이용하여 실제 수행 command는 뒤에 옵션이 추가로 붙습니다.)</p>

<p><br></p>

<h4>Result</h4>

<p>수행 결과로 다음과 같은 결과가 출력됩니다.</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">(0,(34,0.9846535656842613))
(0,(96,0.8178838683876802))
...
(1,(96,1.2547672185210839))
(1,(4,1.941481009392396))
...
(29,(86,1.0588376599353693))
(29,(68,3.3195965377284837))
</code></pre></div>
<p>위 결과는 각각의 사용자 0 ~ 29에 대해 영화별 평점을 예측한 수치입니다.</p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->



<div class="pagination">
  <ul class="inline-list">
    
    
      
        <li><a href="/page2/" class="btn">Previous</a></li>
      
    

    
    
      <li><a href="">1</a></li>
    

    
    

    
    
    

    
      
        
        
        
        <li><a href="/page2/">2</a></li>
      
    
      
        <li><strong class="current-page">3</strong></li>
      
    

    
    

    
      <li><a href="/page4/">4</a></li>
    

    
    
      <li><a href="/page4/" class="btn">Next</a></li>
    
  </ul>
</div>

</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2015 Hyunje Jo.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="/assets/js/scripts.min.js"></script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-62281776-2', 'auto');
  ga('send', 'pageview');

</script>


<!-- Mathjax -->
<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


          

</body>
</html>